---
title: "<h1>Who do we think we are:</h1> <h3>Overview of Dataset Preparation</h3>"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
author: "Kelly Tall"
date: "15/08/2020"


output: 
        html_document:
                df_print: paged        
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# install.packages("WikidataR")
# install.packages("WikipediR")
# install.packages("tidyverse")
# install.packages("rlist")
# install.packages("WikidataQueryServiceR")


library(tidyverse)
library(rlist)



# load data ---------------------------------------------------------------

##honours data sets
OAqBday <- read_csv("OA_QueenBDay.csv")
OAausDay <- read_csv("OA_AusDay.csv")

##honours list merged
OAFullList <- bind_rows(OAqBday, OAausDay)

##match with wikidata
wikidataMatch <- read_csv("wikidataMatch.csv") %>% 
  select(-X1)


##Australian match with wikidata
wikidataAusMatch <- read_csv("wikidataAusMatch.csv")

##match with wikipedia

wikipediaMatch <- read_csv("wikipediaMatch.csv") 



pageCreation <- read.csv("pageCreation.csv")
  









```

### Project Summary and Aims

The two core analysis aims are:

1. Capture the number of wikipedia pages created for people who have are recipients of an Order of Australia.
* what is the proporation on those holding an honour who have a wikipedia page v those who do not have a page.
  + examine by Order held
  + examine by state 



2. For whose who have a wikipedia page, capture the date of creation of their page
* illustrate the proportions of pages created pre and post award
  
<i>(any more to add?)</i> 

### Summary of process  

1. Extracted all Order of Australia records from <a href="https://honours.pmc.gov.au/honours/search">The Australian Honours Search Facility</a><br>
2. Matched names of award recipients with all wikidata records using R package <a href="https://cran.r-project.org/web/packages/WikidataR/WikidataR.pdf">wikidataR</a>. This yielded TO DO: (insert # of records)<br>
3. sorted all matches into three "buckets" <br> 
   i) all items that have some "Australianess" in their decsription 
   ii) all items that have "not-Australianess" in their description 
   iii) all items that have neither "Australianess" or "not-Australianess" in their description  
4. Items in the "neither" bucket where manually checked to see if they were a match to an Order of Australian recipient and moved either into the "Australianess" bucket or the "not-Australianess" <br> 
5. Dupliacte records were then extracted from the "Australianess" bucket, were a name from the Order of Australia list was matched with a name of two or more wikidata entries. This was sorted manually by comparing the description from wikidata with the description of the merit. <br> 
6. Part of the "Australianess" bucket was then tested for errors. Again, the description from wikidata was compared against the descipriton of the award from the
Order of Australia list. From 100 enries selected, there were five incorrectly matched, and these were removed. <br> 
7. This list was then finalised, and the wikidataID was used in a scraper to extract the linked wikipedia article and article ID from wikidata <b>TO DO: need to check wiht Prue or Toby: is assumption correct that if a wikipedia page exists, there will be a wikidata entry, AND there will be a wikipedia page listed in the wikidata record)</b> <br> 
8. The edits of each matched wikipedia page were then scraped to extract the page creation date <br> 

<b>To do: insert table showing tally of records for each stage</b>

Of the <b>`r nrow(OAFullList)`</b> records extracted from the honours data base, the match with a wikiData extry with a connection to Australia was <b>`r nrow(wikidataAusMatch)`</b>. 

Once this was filtered for matches to a wikipedia page, there was a match of <b>`r nrow(wikipediaMatch)`</b> articles


### Honours Data source and Extraction 

The honours data set was downloaded from <a href="https://honours.pmc.gov.au/honours/search">The Australian Honours Search Facility</a> published by the Department of the Prime Minster and Cabinet.

The records were extracted from this database for all of the Order of Australia Awards issued since 1975, and extracted based on the following award levels:
        
1. Dame of the Order of Australia 
2. Knight of the Order of Australia  
3. Companion (AC) 
4. Officer (AO) 
5. Member (AM) 
6. Medal (OAM) 

More information about the Order of Australia can be found here: https://en.wikipedia.org/wiki/Order_of_Australia

The data set represents a total of <b>`r nrow(OAFullList)`</b> hounours with each row of the data set an individual reward recipient.

The data variables in our set are:

```{r , echo=FALSE}
colnames(OAFullList)

```

The data extracted is shown for the first few rows of the data set as below:

```{r echo=FALSE, rows.print=7}
head(OAFullList)
```


### Merging Honours with Wikidata information

Names of Order of Australia recipients were then passed through wikidata to gather more information, such as description of the person and their aliases. 

The reason for this was to ensure that names (normally awarded to the recipient with their full name), could also be matched to any other names they are known by. The query was run using an R package (<a href="https://cran.r-project.org/web/packages/WikidataR/WikidataR.pdf">wikidataR</a>) that accesses the wikiData API and matches not only the name the award is given to, but also any alias that is listed in a wikiData entry.  For example, Bob Hawke is listed as;

<b>Bob Hawke</b>

* Description:
  + Australian politician, 23rd Prime Minister of Australia

* Also known as: 
  + Bob Hawke
  + The Honourable Bob Hawke
  + Robert James Lee "Bob" Hawke
  + Robertus Iacobus Lee Hawke (latin)
  + Robertus Hawke (latin)
  + 鮑勃·霍克 (chinese)
        
For his AC, he was named as "Mr Robert James Lee HAWKE". The API searches against his name and aliases to give us a match.

This match of Order of Australia recipients with a wikiData match returned the following information:

```{r echo=FALSE, cols.print =4 ,rows.print=7}
head(wikidataMatch)
```

The description provided in the wikidata record often incuded key words such as "Australian". Using these as a starting point, this data set was filtered to include any mention of "Australian", as well as other key words or phrases such as "Queensland", "Tasmania", "New South Wales" etc. Likewise, other words such as "United States", "Dutch", "Spanish" etc were excluded from the list in the absensce of an Australian related term. 

Once this filtering and matching was done based on the decsription field, there was a list of "unallocated" records that I sifted through manually, and allocated to the Australian list if there was a match. This was done using other information contained in the wikiData entry or in the Honours information.

A final edit was undertaken to remove cases that referred to other "non-person" items such as parks, ovals, reserves, artciles, discographies, filmographies, foundations etc that may have included the name of the award recipient. 

The information is displayed below showing the head of the data set.   

```{r echo=FALSE, cols.print =4 ,rows.print=7}
head(wikidataAusMatch)
```

### Matching names to wikipedia pages

Using the list of Australian matches, the wikidata ID was used in a scraper to get the wikipedia page url and article ID from wikidata. This gave us <b>`r nrow(wikipediaMatch)`</b> wikipedia article links.


```{r echo=FALSE, cols.print =2 ,rows.print=7}
head(wikipediaMatch)
```


### Extracting a page creation date  

Each wikipedia page match has also been linked to a page creation date.

https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvlimit=1&rvprop=timestamp&rvdir=newer&pageids=Q2075218 

The above query asks to sort all revisions from oldest to most recent, and pull top timestamp , for page ID 2352403. Using a loop function, this query was scarped using the wikipedia ID of each matched article, and the timstamp was recorded.  

(This query was found via a search on <a href="https://stackoverflow.com/questions/43898352/how-to-get-date-of-creation-of-wikipedia-page-by-api">stackoverflow</a> and I built a simple scraper to store the time stamp against the page id.) 

```{r echo=FALSE, cols.print =4 ,rows.print=7}
head(pageCreation)
```


### Final data merge

All data sets were then merged together, into a final data set.


Kelly To do:

merge honours to wikipage file with creation date
Fields to include:

* fullName
* honours ID
* honours level
* honours state
* wikimediaID
* wikipediaID
* page creation date


<br>

### Data set and process limitations

There are pros and cons to this method. It speeds up a manual process of checking if the matched records are Order of Australia award recipients. It also means that inadvertantly a record may have been included that may not have been an Order of Australia recipient, but had a name and text identifyer (such as "Australian", "Queensland" etc) match. 

For example Bob Smith has received an AM. He has no wikiData entry. Bob Smith does not have an AM, but has a wikiData entry and a description that says "Australian medical researcher". The second Bob Smith  will be included in the list that is matched to the wikipedia query, even though he has no award. If we also has a wikipedia page he will be included in the final data set.

If an award recipient's description included another country but did not mention "Australia" or other Australian related terms, it will not included in the list. For example Jane A Smith has a wikidata entry. She has receievd an OA. Her decription says "Italian-born artist". She would be excluded from our list based on the presence of "Italian" without an Australian qualifier. If Jane Smith has an OA and is described as an "Italian-born Australian artist" she is included on our list.

It is hypothesised that these examples are the exception rather than the rule, and the majorty of matching cases identified in process are correct. A manual check of approx 500 cases resulted in five match errors. 

